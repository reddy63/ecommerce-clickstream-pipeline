Ecommerce Clickstream Data Pipeline

A production-style end-to-end Data Engineering pipeline built using Kafka, Spark, HDFS, Airflow, PostgreSQL, Docker, and Metabase.

This project simulates real-time ecommerce clickstream data ingestion, transformation using the Medallion Architecture (Bronze â†’ Silver â†’ Gold), and serves analytics-ready data to PostgreSQL for dashboard visualization.

ğŸ“Œ Architecture Overview
FakeStore API
     â†“
Kafka (Streaming Ingestion)
     â†“
Spark Structured Streaming (Bronze Layer â†’ HDFS)
     â†“
Spark Batch (Silver Layer - Clean & Deduplicate)
     â†“
Spark Aggregation (Gold Layer - Business Metrics)
     â†“
PostgreSQL (Analytics DB)
     â†“
Metabase (Dashboard)

ğŸ— Tech Stack
Layer	Technology
Streaming	Apache Kafka
Processing	Apache Spark (Structured Streaming + Batch)
Storage	HDFS
Orchestration	Apache Airflow
Database	PostgreSQL
Visualization	Metabase
Containerization	Docker & Docker Compose

ğŸ“‚ Project Structure
ecommerce-clickstream-pipeline/
â”‚
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ clickstream_pipeline_dag.py
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ spark-jobs/
â”‚   â”œâ”€â”€ silver_clickstream.py
â”‚   â”œâ”€â”€ gold_clickstream.py
â”‚   â””â”€â”€ gold_to_postgres.py
â”‚
â”œâ”€â”€ spark-streaming/
â”‚   â”œâ”€â”€ clickstream_streaming.py
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ producer/
â”‚   â”œâ”€â”€ producer.py
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ docker-compose.infra.yml
â”œâ”€â”€ docker-compose.app.yml
â”œâ”€â”€ docker-compose.airflow.yml
â”œâ”€â”€ .env
â””â”€â”€ README.md

ğŸ¥‰ Bronze Layer (Raw Data)

Source: https://fakestoreapi.com/products

Data ingested via Kafka

Spark Structured Streaming writes raw JSON â†’ HDFS (Parquet)

Partitioned storage

Checkpointing enabled

Schema:

{
  "product_id": int,
  "category": string,
  "price": double,
  "event_ts": long
}

ğŸ¥ˆ Silver Layer (Clean & Transform)

Convert epoch â†’ timestamp

Add event_date

Deduplicate records

Partition by event_date

Stored in HDFS

ğŸ¥‡ Gold Layer (Business Metrics)

Aggregated KPIs:

Total Events per Category

Total Revenue per Category

Grouped by event_date

Example Output:

category	total_events	total_revenue	event_date
electronics	54	17954.91	2026-02-17
ğŸ—„ PostgreSQL Analytics Table

Table: clickstream_metrics

CREATE TABLE clickstream_metrics (
    category TEXT,
    total_events INT,
    total_revenue DOUBLE PRECISION,
    event_date DATE,
    PRIMARY KEY (category, event_date)
);


Loaded using Spark JDBC connector.

ğŸ¯ Airflow Orchestration

DAG: clickstream_pipeline

Tasks:

silver_layer

gold_layer

gold_to_postgres

Schedule: @hourly

Retry enabled.

ğŸ“Š Dashboard (Metabase)

Connect to PostgreSQL

Create visualizations:

Revenue by Category

Daily Events

Trend analysis

Top categories

Access:

http://localhost:3000

ğŸ³ How to Run the Project
1ï¸âƒ£ Clone Repository
git clone https://github.com/your-username/ecommerce-clickstream-pipeline.git
cd ecommerce-clickstream-pipeline

2ï¸âƒ£ Create .env File

Example:

POSTGRES_HOST=postgre
POSTGRES_USER=arun
POSTGRES_PASSWORD=yourpassword
POSTGRES_DB=airflow
POSTGRES_DB1=analytics
POSTGRES_PORT=5432

3ï¸âƒ£ Start Infrastructure
docker compose -f docker-compose.infra.yml up -d

4ï¸âƒ£ Start Streaming App
docker compose -f docker-compose.app.yml up -d --build

5ï¸âƒ£ Start Airflow
docker compose -f docker-compose.airflow.yml up -d --build


Airflow UI:

http://localhost:8081

ğŸ”¥ Key Engineering Concepts Demonstrated

âœ” Medallion Architecture
âœ” Real-time Streaming
âœ” Batch Processing
âœ” Distributed Storage (HDFS)
âœ” DAG Orchestration
âœ” Spark JDBC Integration
âœ” Containerized Microservices
âœ” Environment Variable Management
âœ” Production-style Project Structure

ğŸ“ˆ Scalability Improvements (Future Enhancements)

Replace LocalExecutor â†’ CeleryExecutor

Add Delta Lake

Add Data Quality checks

Add CI/CD pipeline

Deploy to AWS (EMR + MSK + RDS)

Add dbt for transformations

ğŸ‘¨â€ğŸ’» Author

Arun Reddy
Entry-Level Data Engineer
Tech: Python | SQL | Spark | Kafka | Airflow | AWS

â­ Why This Project Matters

This project demonstrates:

End-to-end pipeline thinking

Real-world orchestration

Distributed systems understanding

Production-ready architecture

Strong foundation for Data Engineering roles